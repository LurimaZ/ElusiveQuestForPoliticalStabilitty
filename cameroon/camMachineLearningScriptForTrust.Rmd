---
title: "Cameroon: Machine Learning Script for the Predition of Trust "
author: "Adrien Ratsimbaharison"
date: "May 26, 2019"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This machine learning script is based on the "Practical guide to implement machine learning with CARET in R" suggested by Saurav Kaushik in the Analytics Vidhya website: https://www.analyticsvidhya.com/blog/2016/12/practical-guide-to-implement-machine-learning-with-caret-package-in-r-with-practice-problem/ (December 8, 2016).   

The Caret package solves many problems in the implementation of machine learning process, and make it much easier to execute, even for beginners like the autors of this book.
As Kaushik puts it, "This package [i.e., CARET package] alone is all you need to know for solve almost any supervised machine learning problem. It provides a uniform interface to several machine learning algorithms and standardizes various other tasks such as Data splitting, Pre-processing, Feature selection, Variable importance estimation, etc."

Among the different steps suggested by Saurav Kaushik, we will focus on the following:  

1) Getting started with loading the package and reading the data
2) Pre-processing the data using Caret
3) Splitting the data using Caret
4) Feature selection using Caret
5) Training models using Caret
6) Variable importance estimation using Caret
7) Making predictions using Caret


## 1. Getting started with loading the package and looking at the data

Installing and loading the Caret package and its dependencies:

```{r message=FALSE, warning=FALSE, include=FALSE}
# Intalling the caret package if it is not already installed

# install.packages("caret", dependencies = c("Depends", "Suggests"))

# loading the caret package and other required packages:
library(caret)
library(dplyr)
library(haven)
library(magrittr)
library(gbm)

```

Reading the data in R and looking at its structure:

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Reading the data
trustData <- read_sav("cam_r7_data_eng.sav")
trustData <- as_tibble(trustData)

# Looking at its structure
str(trustData)

# Selecting the variables of interest

trustData <- select(trustData, URBRUR, REGION, EA_SVC_A, EA_SVC_B, EA_SVC_C, EA_SVC_D, EA_FAC_A, EA_FAC_B, EA_FAC_C, EA_FAC_D, EA_FAC_E, EA_FAC_F, EA_FAC_G, EA_SEC_A, EA_SEC_B, EA_SEC_C, EA_SEC_D, EA_SEC_E, EA_ROAD_A, EA_ROAD_B, EA_ROAD_C, Q1, Q2A, Q2AOTHER, Q2B, Q2BOTHER, Q3, Q4A, Q4B, Q5, Q6, Q7, Q8A, Q8B, Q8C, Q8D, Q8E, Q8F, Q9, Q10A, Q10B, Q11A, Q11B, Q12A, Q12B, Q12C, Q12D, Q12E, Q13, Q14, Q15, Q16, Q17, Q18A, Q18B, Q18C, Q18D, Q19A, Q19B, Q19C, Q19D, Q19E, Q20A, Q20B, Q21A, Q21B, Q22, Q23, Q24A, Q24B, Q25A, Q25B, Q25C, Q25D, Q25E, Q25F, Q26A, Q26B, Q26C, Q26D, Q26E, Q27A, Q27B, Q27C, Q28, Q29, Q30, Q31, Q32, Q33, Q34, Q35, Q36, Q37, Q38A, Q38B, Q38C, Q38D, Q38E, Q38F, Q38G, Q39A, Q39B, Q39C, Q40, Q41, Q42A, Q42B, Q42C, Q42D, Q42E, Q42F, Q43A, Q43B, Q43C, Q43D, Q43E, Q43F, Q43G, Q43H, Q43I, Q43J, Q43K, Q43L_CAM, Q43M_CAM, Q44A, Q44B, Q44C, Q44D, Q44E, Q44F, Q44G, Q44H, Q44I, Q44J, Q44K_CAM, Q44L_CAM, Q44M_CAM, Q44N_CAM, Q45, Q46, Q47, Q48A, Q48B, Q48C, Q48D, Q48E, Q48F, Q49A, Q49B, Q49C, Q49D, Q49E, Q49F, Q49G, Q49H, Q49I, Q49J, Q49K, Q49L, Q49M, Q49N, Q49O, Q49P, Q49Q, Q49R, Q49S, Q49T, Q50, Q51, Q52, Q53A, Q53B, Q53C, Q54A, Q54B, Q55PT1, Q55PT1OTHER, Q55PT2, Q55PT2OTHER, Q55PT3, Q55PT3OTHER, Q56A, Q56B, Q56C, Q56D, Q56E, Q56F, Q56G, Q56H, Q56I, Q56J, Q56K, Q56L, Q56M, Q56N, Q56O, Q56P, Q56Q, Q56R, Q56S, Q57A, Q57B, Q57C, Q57D, Q57E, Q57F, Q57G, Q58A, Q58B, Q58C, Q58D, Q58E, Q59A, Q59B, Q59C, Q60A, Q60B, Q60C, Q60D, Q61A, Q61B, Q61C, Q62, Q63, Q64, Q65, Q66, Q67, Q68A, Q68B, Q69, Q70, Q70OTHER, Q71, Q72A, Q72B, Q73A, Q73B, Q74, Q75, Q76, Q77A, Q77B, Q77C, Q77D, Q78A, Q78B, Q79_CAM, Q80_CAM, Q81A_CAM, Q81B_CAM, Q81C_CAM, Q81D_CAM, Q82A_CAM, Q82B_CAM, Q82C_CAM, Q82D_CAM, Q84, Q84OTHER, Q85A, Q85B, Q86A, Q86B, Q86C, Q86D, Q87A, Q87B, Q87C, Q87D, Q88A,Q88B, Q88BOTHER, Q89A, Q89B, Q89C, Q89D, Q89E, Q89F, Q90, Q91A, Q91B, Q92A, Q92B, Q93, Q94, Q95A, Q95B, Q95C, Q96A, Q96B, Q97, Q98, Q98OTHER, Q99, Q99OTHER, Q100)

# Changing the types of all variables to factor, except the variable "age" which should be kept numeric
trustData %<>% 
      mutate_each(funs(if(is.numeric(.)) as.factor(.) else .))

trustData$Q1 <- as.numeric(trustData$Q1)

# we need also to remove the variables that end with "OTHER" which created the errors "NA/NaN/Inf in foreign function call (arg 1)" in the execution of feature selection using the "rfe" function in step 4;

trustData <- select(trustData, - c(Q2AOTHER, Q2BOTHER, Q55PT3OTHER, Q55PT2OTHER, Q55PT1OTHER, Q70OTHER, Q98OTHER, Q84OTHER, Q88BOTHER, Q99OTHER))

```


Defining the problem:

In this problem we want to predict the classification of Cameroonians into three categories, according to their responses to Question Q43A in Afrobarometer Round 7 survey ("How much do you trust the President?"):

- "Distrut the President"
- "Trust the President"
- "Don't know whether they trust or not the President"

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

# In order to simplify the interpretation of the results of the machine learning implementation, we need to create the variable "trust" with three categories of responses, by mutating Q43A and assigning the following value: 0 = "Distrust", 1 = "Trust", 2 = "Don't know"
trustData <- mutate(trustData, trust = ifelse(Q43A == 0 | Q43A == 1, "0",
                                           ifelse(Q43A == 2 | Q43A == 3, "1", "2" )))

trustData$trust <- as.factor(trustData$trust)


# we remove Q43A from the dataset, as it is redundant with the new variable approval
trustData <- select(trustData, - Q43A)
trustData_transformed <- trustData

```


## 2. Pre-processing the data using Caret

In this step, we only check for the missing values, and skipped the other pre-processing procedures, such as centering, scaling, principal component analysis, and creation of "one hot encoding". 

```{r}
sum(is.na(trustData_transformed))

```

Since there is no missing value to remove, we can move to the next step, which is splitting the data.



## 3. Splitting the data using Caret

Since the dataset was initially arranged in a country-year format, it is a good idea to randomize the rows before splitting the dataset into trainSet and testSet. 

```{r}
# Randomizing the dataset
set.seed(222)
rows <- sample(nrow(trustData_transformed))
trustData_transformed <- trustData_transformed[rows,]

#Spliting dataset into trainSet and testSet based on outcome with a ratio of 75% and 25%, using createDataPartition in Caret
index <- createDataPartition(trustData_transformed$trust, p=0.75, list=FALSE)
trustTrainSet <- trustData_transformed[index,]
trustTestSet <- trustData_transformed[-index,]

#Checking the structure of trustTrainSet
str(trustTrainSet)

```



## 4. Feature selection using Caret

The feature selection, which is a crucial part of machine learning, is made easy by Caret. The "recursive feature elimination" or "rfe" function in Caret is used to find the best subset of features to be included in the models.

```{r}
# Feature selection using rfe in caret
ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 3,
                   verbose = FALSE)

y <- trustTrainSet$trust
x <- select(trustTrainSet, - trust)

trustProfile <- rfe(x, y,
                rfeControl = ctrl)

trustProfile

```
Results:

Recursive feature selection

Outer resampling method: Cross-Validated (10 fold, repeated 3 times) 

Resampling performance over subset size:


The top 5 variables (out of 286):
   Q43B, Q43C, Q43E, Q43L_CAM, Q43H


## 5. Training models using Caret

Caret provides a large number of algorithms with similar syntax. Following Kaushik's practical guide, we will apply the following: GBM and Random forest.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3)
trustModel_gbm <- train(trust ~ ., data = trustTrainSet, 
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE)

trustModel_rf <- train(trust ~ ., data = trustTrainSet, 
                 method = "rf", 
                 trControl = fitControl,
                 verbose = FALSE,
                 importance=T)


```


## 6. Variable importance estimation using Caret

We can also check the variable importance estimates in Caret by using the "varImp" function" for any model.

### 6.1 Variable importance for GBM

```{r}
# Checking variable importance for GBM
# Variable Importance
varImp(object=trustModel_gbm)


#Plotting Varianle importance for GBM
plot(varImp(object=trustModel_gbm),main="GBM - Variable Importance", top = 20)

```

### 6.2 Variable importance for RF

```{r}
#Checking variable importance for RF
varImp(object=trustModel_rf)
#rf variable importance


#Plotting Varianle importance for Random Forest
plot(varImp(object=trustModel_rf),main="RF - Variable Importance", top = 20)

```


## 7. Making predictions using Caret

```{r}
#Predictions with gbm
trustPrediction_gbm <-predict.train(object=trustModel_gbm,trustTestSet,type="raw")
table(trustPrediction_gbm)

confusionMatrix(trustPrediction_gbm,trustTestSet$trust)


```

trustPrediction_gbm
  0   1   2 
101 189  11 
> 
> confusionMatrix(trustPrediction_gbm,trustTestSet$trust)
Confusion Matrix and Statistics

          Reference
Prediction   0   1   2
         0  73  19   9
         1  30 157   2
         2   2   2   7

Overall Statistics
                                          
               Accuracy : 0.7874          
                 95% CI : (0.7368, 0.8322)
    No Information Rate : 0.5914          
    P-Value [Acc > NIR] : 4.492e-13       
                                          
                  Kappa : 0.5826          
                                          
 Mcnemar's Test P-Value : 0.07436         

Statistics by Class:

                     Class: 0 Class: 1 Class: 2
Sensitivity            0.6952   0.8820  0.38889
Specificity            0.8571   0.7398  0.98587
Pos Pred Value         0.7228   0.8307  0.63636
Neg Pred Value         0.8400   0.8125  0.96207
Prevalence             0.3488   0.5914  0.05980
Detection Rate         0.2425   0.5216  0.02326
Detection Prevalence   0.3355   0.6279  0.03654
Balanced Accuracy      0.7762   0.8109  0.68738
> 

```{r}
#Predictions with rf
trustPrediction_rf <-predict.train(object=trustModel_rf,trustTestSet,type="raw")
table(trustPrediction_rf)

confusionMatrix(trustPrediction_rf,trustTestSet$trust)

```

> confusionMatrix(trustPrediction_rf,trustTestSet$trust)
Confusion Matrix and Statistics

          Reference
Prediction   0   1   2
         0  63  13   7
         1  41 165   3
         2   1   0   8

Overall Statistics
                                          
               Accuracy : 0.7841          
                 95% CI : (0.7332, 0.8292)
    No Information Rate : 0.5914          
    P-Value [Acc > NIR] : 1.146e-12       
                                          
                  Kappa : 0.5606          
                                          
 Mcnemar's Test P-Value : 6.465e-05       

Statistics by Class:

                     Class: 0 Class: 1 Class: 2
Sensitivity            0.6000   0.9270  0.44444
Specificity            0.8980   0.6423  0.99647
Pos Pred Value         0.7590   0.7895  0.88889
Neg Pred Value         0.8073   0.8587  0.96575
Prevalence             0.3488   0.5914  0.05980
Detection Rate         0.2093   0.5482  0.02658
Detection Prevalence   0.2757   0.6944  0.02990
Balanced Accuracy      0.7490   0.7846  0.72046
> 
> 
## Conclusion
